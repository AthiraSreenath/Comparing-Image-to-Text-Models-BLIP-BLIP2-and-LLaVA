{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2961388-c9a0-40b2-934f-0f3df947cf1a",
   "metadata": {},
   "source": [
    "## Demo of LLaVA model for image description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4eb58-481d-4801-98cf-743f86d3e62d",
   "metadata": {},
   "source": [
    "#### Install LLaVA from official Git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3415a9-f8d4-4a62-888f-f27162077dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone https://github.com/haotian-liu/LLaVA and run the following\n",
    "# cd LLaVA\n",
    "# ! pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1acb407-6d0b-4a87-b5db-5ece43a206ca",
   "metadata": {},
   "source": [
    "#### Do the necessary installs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0d5102-0fbb-4e73-90dc-ac4b65c7e17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install torch==2.1.0 torchvision==0.16.0\n",
    "# ! pip uninstall bitsandbytes -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af976a-46a3-4323-8e3f-cb550f907e9b",
   "metadata": {},
   "source": [
    "#### Import all the neccesary packages and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceddc947-862c-464a-902b-262de35e276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from llava.serve.cli import load_image\n",
    "from llava.constants import IMAGE_TOKEN_INDEX\n",
    "from llava.model.builder import load_pretrained_model\n",
    "from llava.mm_utils import tokenizer_image_token, get_model_name_from_path, KeywordsStoppingCriteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4ed97-5825-46db-b488-1392572fc2fd",
   "metadata": {},
   "source": [
    "#### Load the pretrained model and process a given image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "577f6fda-724a-4532-ac92-e55d0d30daf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94cf203af5447ef9c9ddb4276a1e6d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"liuhaotian/llava-v1.5-7b\"\n",
    "model_base = None\n",
    "device = \"cuda\"\n",
    "image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "\n",
    "model_name = get_model_name_from_path(model_path)\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(model_path, model_base, model_name, device=device)\n",
    "\n",
    "image = load_image(image_file)\n",
    "image_tensor = image_processor([image], return_tensors='pt')['pixel_values']\n",
    "image_tensor = image_tensor.to(model.device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ec33c-2ce6-4549-84f9-abdfb2c4ad31",
   "metadata": {},
   "source": [
    "#### Generate response based on the input prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c286a65-09ff-4982-9e6c-4b38cbb41ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  The image features a pier extending over a large body of water, possibly a lake or a river. The pier is made of wood and has a bench situated on it, providing a place for people to sit and enjoy the view. The surrounding area is filled with trees, creating a serene and peaceful atmosphere.\n",
      "\n",
      "In the background, there are mountains visible, adding to the picturesque scenery. The pier is located near the water's edge, allowing visitors to appreciate the beauty of the landscape and the tranquility of the water.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image> Can you describe this picture in detail?\\nASSISTANT:\"\n",
    "\n",
    "input_ids = tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)\n",
    "stopping_criteria = KeywordsStoppingCriteria([\"</s>\"], tokenizer, input_ids)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        images=image_tensor,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        max_new_tokens=512,\n",
    "        use_cache=True,\n",
    "        stopping_criteria=[stopping_criteria])\n",
    "\n",
    "output = tokenizer.decode(output_ids[0, input_ids.shape[1]:]).strip()\n",
    "print(\"Output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962f7a9-def2-4b6d-84dd-2ed2ce7ee6cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
